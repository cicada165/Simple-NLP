{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264ec5d6",
   "metadata": {},
   "source": [
    "# AML Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d8ff3",
   "metadata": {},
   "source": [
    "## Programming Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab91fb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/qu4ntum/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/qu4ntum/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/qu4ntum/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057c8cc9",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb96f88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data have 7613 data points.\n",
      "Testing data have 3263 data points.\n",
      "Percentage of real disasters: 0.43\n",
      "Percentage of not real disasters: 0.57\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "print(\"Training data have \" + str(train.shape[0]) + \" data points.\")\n",
    "print(\"Testing data have \" + str(test.shape[0]) + \" data points.\")\n",
    "\n",
    "num_fake = train['target'].value_counts(dropna=False)[0]\n",
    "num_real = train['target'].value_counts(dropna=False)[1]\n",
    "\n",
    "print(\"Percentage of real disasters: \" + str(round(num_real/(num_real+num_fake),3)))\n",
    "print(\"Percentage of not real disasters: \" + str(round(num_fake/(num_real+num_fake),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58388d7",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f17cf164",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.iloc[:,:-1]\n",
    "y = train.iloc[:,-1:]\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918417f",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea09057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    stopword = stopwords.words('english')\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    #Tokenize text\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    #Convert to lowercase\n",
    "    word_tokens = [w.lower() for w in word_tokens]\n",
    "    #Lemmatization and stripping stopwords\n",
    "    word_tokens = [wordnet_lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "    #Strip punctuation,@,and urls\n",
    "    word_tokens = [w for w in word_tokens if w not in punctuation]\n",
    "    return word_tokens\n",
    "\n",
    "\n",
    "tmp = []\n",
    "for text in train['text']:\n",
    "    tmp.append(preprocess(text))\n",
    "train['text'] = tmp\n",
    "\n",
    "tmp = []\n",
    "for text in test['text']:\n",
    "    tmp.append(preprocess(text))\n",
    "test['text'] = tmp\n",
    "\n",
    "x = train.iloc[:,:-1]\n",
    "y = train.iloc[:,-1:]\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size=0.30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01755e36",
   "metadata": {},
   "source": [
    "### (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2015eb13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "len not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-234ca05eedc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mx_dev_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#Bag of words for training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of feature for training set Bag of Words vector: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_bow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of feature for development set Bag of Words vector: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dev_bow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: len not found"
     ]
    }
   ],
   "source": [
    "def token_to_sentence(df):#Convert the tokenized 'text' field back to sentence in order to be vectorized\n",
    "    tmp = []\n",
    "    for sentence in df['text']:\n",
    "        str = ''\n",
    "        for token in sentence:\n",
    "            str += token + ' '\n",
    "        tmp.append(str)\n",
    "    return tmp\n",
    "        \n",
    "train_text = token_to_sentence(x_train)\n",
    "dev_text = token_to_sentence(x_dev)\n",
    "test_text = token_to_sentence(test)\n",
    "\n",
    "\n",
    "x_train['text'] = train_text\n",
    "x_dev['text'] = dev_text\n",
    "test['text'] = test_text\n",
    "\n",
    "# vectorizer = CountVectorizer(binary=True, min_df=10)\n",
    "# vectorizer.fit(x_train[\"text\"])\n",
    "\n",
    "# x_train_bow = vectorizer.transform(x_train['text'])\n",
    "# x_dev_bow = vectorizer.transform(x_dev['text'])\n",
    "\n",
    "M = 0\n",
    "max_score = -9999\n",
    "\n",
    "for i in range(50):\n",
    "    vectorizer = CountVectorizer(binary=True, min_df=i)\n",
    "    vectorizer.fit(x_train[\"text\"])#Build a vocabulary of the words appearing in the training set\n",
    "\n",
    "    x_train_bow = vectorizer.transform(x_train['text'])\n",
    "    x_dev_bow = vectorizer.transform(x_dev['text'])\n",
    "\n",
    "    model_logistic = LogisticRegression(penalty='none')\n",
    "    model_logistic.fit(x_train_bow, y_train)\n",
    "\n",
    "    y_dev_predict = model_logistic.predict(x_dev_bow)\n",
    "\n",
    "    if f1_score(y_dev, y_dev_predict) > max_score: \n",
    "        max_score = f1_score(y_dev, y_dev_predict)\n",
    "        M = i\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, min_df=M)\n",
    "vectorizer.fit(x_train[\"text\"])#Build a vocabulary of the words appearing in the training set\n",
    "\n",
    "\n",
    "x_train_bow = vectorizer.transform(x_train['text'])#Bag of words for training data\n",
    "x_dev_bow = vectorizer.transform(x_dev['text'])#Bag of words for training data\n",
    "\n",
    "print(\"Total number of feature for training set Bag of Words vector: \" + str(x_train_bow.len()))\n",
    "print(\"Total number of feature for development set Bag of Words vector: \" + str(x_dev_bow.len()))\n",
    "\n",
    "\n",
    "# x_train['text'] = x_train_bow\n",
    "# x_dev['text'] = x_dev_bow\n",
    "# test['text'] = test_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "027286a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features of these vectors (They should be equal since vectorizer is the same): 15988\n"
     ]
    }
   ],
   "source": [
    "print('Total number of features of these vectors (They should be equal since vectorizer is the same): ' + str(x_train_bow.shape[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a56639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M = 0\n",
    "# max_score = -9999\n",
    "\n",
    "\n",
    "# for i in range(100):\n",
    "#     vectorizer = CountVectorizer(binary=True, min_df=i)\n",
    "#     vectorizer.fit(x_train[\"text\"])\n",
    "\n",
    "#     x_train_bow = vectorizer.transform(x_train['text'])\n",
    "#     x_dev_bow = vectorizer.transform(x_dev['text'])\n",
    "\n",
    "#     model_logistic = LogisticRegression(penalty='none')\n",
    "#     model_logistic.fit(x_train_bow, y_train)\n",
    "\n",
    "#     y_dev_predict = model_logistic.predict(x_dev_bow)\n",
    "\n",
    "#     if f1_score(y_dev, y_dev_predict) > max_score: \n",
    "#         max_score = f1_score(y_dev, y_dev_predict)\n",
    "#         M = i\n",
    "\n",
    "# print(\"Optimum M is: \" + str(M))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64d2d9",
   "metadata": {},
   "source": [
    "### (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ec2a5",
   "metadata": {},
   "source": [
    "#### i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3706a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic = LogisticRegression(penalty='none')\n",
    "model_logistic.fit(x_train_bow, y_train)\n",
    "\n",
    "y_train_predict = model_logistic.predict(x_train_bow)\n",
    "print('F1-Score for training set without regularization: ' + str(f1_score(y_train, y_train_predict)))\n",
    "\n",
    "y_dev_predict = model_logistic.predict(x_dev_bow)\n",
    "print('F1-Score for development set without regularization: ' + str(f1_score(y_dev, y_dev_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ac146",
   "metadata": {},
   "source": [
    "#### ii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e299390",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_logistic_L1 = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "model_logistic_L1.fit(x_train_bow, y_train)\n",
    "\n",
    "y_train_predict = model_logistic_L1.predict(x_train_bow)\n",
    "print('F1-Score for training set with L1 regularization: ' + str(f1_score(y_train, y_train_predict)))\n",
    "\n",
    "y_dev_predict = model_logistic_L1.predict(x_dev_bow)\n",
    "print('F1-Score for development set with L1 regularization: ' + str(f1_score(y_dev, y_dev_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f0f33e",
   "metadata": {},
   "source": [
    "#### iii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98716bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic_L2 = LogisticRegression(penalty='l2')\n",
    "model_logistic_L2.fit(x_train_bow, y_train)\n",
    "\n",
    "y_train_predict = model_logistic_L2.predict(x_train_bow)\n",
    "print('F1-Score for training set with L2 regularization: ' + str(f1_score(y_train, y_train_predict)))\n",
    "\n",
    "y_dev_predict = model_logistic_L2.predict(x_dev_bow)\n",
    "print('F1-Score for development set with L2 regularization: ' + str(f1_score(y_dev, y_dev_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb95fb5",
   "metadata": {},
   "source": [
    "#### iv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb66b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The logistic classifier without any regularization performed the best on training set.\n",
    "#The logistic classifier with L2 regularization performed the best on development set.\n",
    "#Yes there is overfitting across all three logistic classifier since the training set performance is higher than \n",
    "#development set performance\n",
    "#However, regularization does apeart to reduce overfitting since the gap between performance on training and development\n",
    "#is shrinked after regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb23f5a",
   "metadata": {},
   "source": [
    "#### v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd0ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weights\n",
    "weights = model_logistic_L1.coef_.flatten()\n",
    "\n",
    "df_weights = pd.DataFrame({'Label': vectorizer.get_feature_names(), 'Weight': weights})\n",
    "df_weights = df_weights.sort_values(by=['Weight'], ascending=False)\n",
    "df_weights.head(5)\n",
    "#Top 3 words: spill, hiroshima, derailment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fef951e",
   "metadata": {},
   "source": [
    "### (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82dd70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2 = x_train_bow.toarray()\n",
    "y_train_arr = y_train[\"target\"].values\n",
    "\n",
    "n = x_train_2.shape[0]\n",
    "d = x_train_2.shape[1]\n",
    "alpha = 1\n",
    "K = 2\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "\n",
    "for k in range(K):\n",
    "    X_k = x_train_2[y_train_arr == k]\n",
    "    psis[k] = np.mean(X_k, axis=0)\n",
    "    phis[k] = (X_k.shape[0] + alpha) / (float(n) + alpha * 2)\n",
    "    \n",
    "def nb_predictions(x, psis, phis):\n",
    "    n, d = x.shape\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    \n",
    "    logpy = np.log(phis).reshape([K,1])\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])\n",
    "\n",
    "y_train_predict, logpyx = nb_predictions(x_train_2, psis, phis)\n",
    "print(\"F1-score for training set is: \" + str(f1_score(y_train, y_train_predict)))\n",
    "y_dev_predict, logpyx2 = nb_predictions(x_dev_bow.toarray(), psis, phis)\n",
    "print(\"F1-score for development set is: \" + str(f1_score(y_dev, y_dev_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a213d383",
   "metadata": {},
   "source": [
    "### (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879cb954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discriminative classifier (Logistic classifier with L2 regularization) performed the best with development set, but \n",
    "#not by much compared to generative classifier. In fact, generative classifier performed better than logistic \n",
    "#classifier without regularization\n",
    "\n",
    "# Generative Model Pros:\n",
    "# Can do more than just prediction: generation, fill-in missing features, etc.\n",
    "# Can include extra prior knowledge; if prior knowledge is correct, model will be more accurate.\n",
    "# Understandability: it is easier to understand the result\n",
    "# Outlier detection: we may detect via  𝑝(𝑥′)  if  𝑥′  is an outlier.\n",
    "# Scalability: Simple formulas for maximum likelihood parameters.\n",
    "# Generation: we can sample  𝑥∼𝑝(𝑥|𝑦)  to generate new data (images, audio)\n",
    "# Generative Model Cons:\n",
    "# Generative model assumes independence which is not always true, causing over or under confident of model\n",
    "# Computationally expensive\n",
    "\n",
    "# Discriminative model Pros:\n",
    "# Most state-of-the-art algorithms for classification are discriminative (including neural nets, boosting, SVMs, etc.)\n",
    "# They are often more accurate because they make fewer modeling assumptions.\n",
    "# Computationally cheaper\n",
    "# Discriminative model Cons:\n",
    "# Cannot generate data \n",
    "# Hard to understand\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed2073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive bayes model assumes words are uncorrelated.\n",
    "#Logistic regression requires the observations to be independent of each other\n",
    "#It is sufficient for bag of words since it is binary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0978fe2",
   "metadata": {},
   "source": [
    "### (h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9a63e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M = 0\n",
    "max_score = -9999\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    vectorizer2 = CountVectorizer(binary=True,min_df=i, ngram_range=(1,2))\n",
    "    vectorizer2.fit(x_train[\"text\"])\n",
    "\n",
    "    x_train_bow_2 = vectorizer2.transform(x_train['text'])\n",
    "    x_dev_bow_2 = vectorizer2.transform(x_dev[\"text\"])\n",
    "\n",
    "    #Regression with N-gram\n",
    "    model_logistic_Ngram = LogisticRegression(penalty='l2')\n",
    "    model_logistic_Ngram.fit(x_train_bow_2, y_train)\n",
    "\n",
    "    y_dev_predict_Ngram = model_logistic_Ngram.predict(x_dev_bow_2)\n",
    "\n",
    "    if f1_score(y_dev, y_dev_predict_Ngram) > max_score: \n",
    "        max_score = f1_score(y_dev, y_dev_predict_Ngram)\n",
    "        M = i\n",
    "\n",
    "#N-Gram models\n",
    "vectorizer2 = CountVectorizer(binary=True,min_df=M, ngram_range=(1,2))\n",
    "\n",
    "#Fitting and processing countVectorizer\n",
    "vectorizer2.fit(x_train[\"text\"])\n",
    "x_train_bow_2 = vectorizer2.transform(x_train[\"text\"])\n",
    "x_dev_bow_2 = vectorizer2.transform(x_dev[\"text\"])\n",
    "\n",
    "dict = vectorizer2.get_feature_names()\n",
    "count_1gram = 0\n",
    "count_2gram = 0\n",
    "for word in dict:\n",
    "    if ' ' in word:\n",
    "        count_2gram += 1\n",
    "    else:\n",
    "        count_1gram += 1\n",
    "        \n",
    "print(\"Number of 1 grams: \" + str(count_1gram))\n",
    "print(\"Number of 2 grams: \" + str(count_2gram))\n",
    "\n",
    "tmp = []\n",
    "for word in dict[1000:]: #some vocabs in the beginning contains numbers so I skipped those\n",
    "    if ' ' in word:\n",
    "        tmp.append(word)\n",
    "    if len(tmp)==10:\n",
    "        break\n",
    "        \n",
    "print(\"Some 2 grams in the vocabulary: \")\n",
    "print(tmp)\n",
    "\n",
    "\n",
    "\n",
    "#N-Gram models\n",
    "vectorizer3 = CountVectorizer(binary=True,min_df=M, ngram_range=(2,2))#2 gram\n",
    "vectorizer3.fit(x_train[\"text\"])\n",
    "x_train_bow_3 = vectorizer3.transform(x_train[\"text\"])\n",
    "x_dev_bow_3 = vectorizer3.transform(x_dev[\"text\"])\n",
    "\n",
    "#Regression with N-gram\n",
    "model_logistic_Ngram = LogisticRegression(penalty='l2')\n",
    "model_logistic_Ngram.fit(x_train_bow_3, y_train)\n",
    "\n",
    "#F-1 score with l2 regularization\n",
    "y_train_predict_Ngram = model_logistic_Ngram.predict(x_train_bow_3)\n",
    "print(\"F1-score for training set with L2 Regularization is: \" + str(f1_score(y_train, y_train_predict_Ngram)))\n",
    "y_dev_predict_Ngram = model_logistic_Ngram.predict(x_dev_bow_3)\n",
    "print(\"F1-score for development set with L2 Regularization is: \" + str(f1_score(y_dev, y_dev_predict_Ngram)))\n",
    "\n",
    "\n",
    "x_train_3 = x_train_bow_3.toarray()\n",
    "\n",
    "n = x_train_3.shape[0]\n",
    "d = x_train_3.shape[1]\n",
    "alpha = 1\n",
    "K = 2\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "\n",
    "for k in range(K):\n",
    "    X_k = x_train_3[y_train_arr == k]\n",
    "    psis[k] = np.mean(X_k, axis=0)\n",
    "    phis[k] = (X_k.shape[0] + alpha) / (float(n) + alpha * 2)\n",
    "    \n",
    "y_train_predict, logpyx3 = nb_predictions(x_train_3, psis, phis)\n",
    "print(\"F1-score for training set is: \" + str(f1_score(y_train, y_train_predict)))\n",
    "y_dev_predict, logpyx4 = nb_predictions(x_dev_bow_3.toarray(), psis, phis)\n",
    "print(\"F1-score for development set is: \" + str(f1_score(y_dev, y_dev_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55287840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M = 0\n",
    "# max_score = -9999\n",
    "\n",
    "\n",
    "# for i in range(50):\n",
    "#     vectorizer2 = CountVectorizer(binary=True,min_df=i, ngram_range=(1,2))\n",
    "#     vectorizer2.fit(x_train[\"text\"])\n",
    "\n",
    "#     x_train_bow_2 = vectorizer2.transform(x_train['text'])\n",
    "#     x_dev_bow_2 = vectorizer2.transform(x_dev[\"text\"])\n",
    "\n",
    "#     #Regression with N-gram\n",
    "#     model_logistic_Ngram = LogisticRegression(penalty='l2')\n",
    "#     model_logistic_Ngram.fit(x_train_bow_2, y_train)\n",
    "\n",
    "#     y_dev_predict_Ngram = model_logistic_Ngram.predict(x_dev_bow_2)\n",
    "\n",
    "#     if f1_score(y_dev, y_dev_predict_Ngram) > max_score: \n",
    "#         max_score = f1_score(y_dev, y_dev_predict_Ngram)\n",
    "#         M = i\n",
    "\n",
    "# print(\"Optimum M for Ngram is: \" + str(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0997e969",
   "metadata": {},
   "source": [
    "### (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = token_to_sentence(train)\n",
    "train['text'] = text\n",
    "y_train = train.iloc[:,-1:]\n",
    "\n",
    "vectorizer.fit(train[\"text\"])\n",
    "test_bow = vectorizer.transform(test['text'])\n",
    "x_train_bow_3 = vectorizer.transform(train[\"text\"])\n",
    "test_bow = vectorizer.transform(test[\"text\"])\n",
    "\n",
    "model_logistic_L2 = LogisticRegression(penalty='l2')\n",
    "model_logistic_L2.fit(x_train_bow_3, y_train)\n",
    "\n",
    "y_train_predict = model_logistic_L2.predict(test_bow)\n",
    "\n",
    "with open(\"submission.csv\",\"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    row = [\"id\", \"target\"]\n",
    "    writer.writerow(row)\n",
    "    for i in range(len(y_train_predict)):\n",
    "        row = [test['id'][i], y_train_predict[i]]\n",
    "        writer.writerow(row)\n",
    "\n",
    "#Kaggle F1-score: 0.79436\n",
    "#This is higher than I expected since the model is performing better on the testing data than the dev data.\n",
    "#It could be that now the training data is about 43% larger because it includes the original development data, therefore,\n",
    "#the model is probably better than before\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36] *",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
